{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Data manipulation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nltk\n", "import pickle\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import numpy as np\n", "import seaborn as sns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from nltk.corpus import stopwords\n", "from nltk.stem import WordNetLemmatizer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.feature_selection import chi2\n", "from sklearn.ensemble import RandomForestClassifier\n", "from pprint import pprint\n", "from sklearn.model_selection import RandomizedSearchCV\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n", "from sklearn.model_selection import ShuffleSplit"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main() -> None:\n", "    #Loading dataset\n", "    df = pd.read_csv(\"dataset.csv\", delimiter=',')\n\n", "    #Building up data for plotting\n", "    categories = df[\"CATEGORY\"].unique()\n", "    data = {}\n", "    for category in categories:\n", "        data[category] = 0\n", "    for cat in df[\"CATEGORY\"]:\n", "        data[cat] += 1\n\n", "    #Getting dataset composition\n", "    names = list(data.keys())\n", "    values = list(data.values())\n", "    \n", "    #Plotting dataset composition\n", "    print(names)\n", "    \n", "    plt.bar(names[0], values[0], color=\"blue\", label=names[0])\n", "    plt.bar(names[1], values[1], color=\"red\", label=names[1])\n", "    plt.bar(names[2], values[2], color=\"black\", label=names[2])\n", "    plt.bar(names[3], values[3], color=\"green\", label=names[3])\n", "    plt.bar(names[4], values[4], color=\"yellow\", label=names[4])\n", "    plt.bar(names[5], values[5], color=\"orange\", label=names[5])\n", "    plt.xlabel(\"Categories\")\n", "    plt.ylabel(\"Size\")\n", "    plt.title(\"Dataset composition\")\n", "    plt.legend()\n", "    plt.grid()\n", "    #plt.show()\n\n", "    #Special characted cleaning\n", "    df[\"F_CONTENT\"] = df[\"F_CONTENT\"].str.replace(\"'s\", \"\")\n", "    df[\"F_CONTENT\"] = df[\"F_CONTENT\"].str.replace(\"\u2019s\", \"\")\n", "    df[\"F_CONTENT\"] = df[\"F_CONTENT\"].str.strip().str.lower().str.replace('\"','')\n\n", "    #TODO: modify punct sign list(?)\n", "    try:\n", "        remove_punct = \"\"\n", "        remove_punct = input(\"Do you want to remove punctuation? [Y/n] \")\n", "        if remove_punct != \"n\":\n", "            for punct_sign in list(\"?:!.,;'\u2019\"):\n", "                df[\"F_CONTENT\"] = df[\"F_CONTENT\"].str.replace(punct_sign, ' ')\n", "    except Exception:\n", "        return\n", "    \n", "    nltk.download('punkt')\n", "    nltk.download('wordnet')\n\n", "    #Lemmatization\n", "    wordnet_lemmatizer = WordNetLemmatizer()\n", "    lemmatized_text_list = []\n", "    for row in range(len(df)):\n", "        lemmatized_list = []\n", "        text = df.loc[row][\"F_CONTENT\"]\n", "        text_words = text.split(\" \")\n", "        for word in text_words:\n", "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n", "        \n", "        lemmatized_text_list.append(\" \".join(lemmatized_list))\n", "    df[\"F_CONTENT\"] = lemmatized_text_list\n", "   \n", "    #Stopwords\n", "    nltk.download('stopwords')\n", "    stop_words = list(stopwords.words('english'))\n", "    for stop_word in stop_words:\n", "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n", "        df[\"F_CONTENT\"] = df[\"F_CONTENT\"].str.replace(regex_stopword, '')\n", "    category_codes = {categories[i]:i for i in range(len(categories))}\n", "    df[\"F_CAT_CODE\"] = [cat.split(\"-\")[0] for cat in df[\"F_NAME\"]]\n", "    df = df.replace({\"F_CAT_CODE\":category_codes})\n", "    \n", "    X_train, X_test, y_train, y_test = train_test_split(df[\"F_CONTENT\"], \n", "                                                    df[\"F_CAT_CODE\"], \n", "                                                    test_size=0.15, \n", "                                                    random_state=8)\n\n", "    #Check with TF-IDF, setting parameters\n", "    ngram_range = (1,2)\n", "    min_df = 0.05\n", "    max_df = 0.90\n", "    max_features = 300\n", "    tfidf = TfidfVectorizer(encoding='utf-8',\n", "                        ngram_range=ngram_range,\n", "                        stop_words=None,\n", "                        lowercase=False,\n", "                        max_df=max_df,\n", "                        min_df=min_df,\n", "                        max_features=max_features,\n", "                        norm='l2',\n", "                        sublinear_tf=True)\n", "                            \n", "    features_train = tfidf.fit_transform(X_train).toarray()\n", "    labels_train = y_train\n", "    print(features_train.shape)\n", "    features_test = tfidf.transform(X_test).toarray()\n", "    labels_test = y_test\n", "    print(features_test.shape)\n", "    for Product, category_id in sorted(category_codes.items()):\n", "        features_chi2 = chi2(features_train, labels_train == category_id)\n", "        indices = np.argsort(features_chi2[0])\n", "        feature_names = np.array(tfidf.get_feature_names())[indices]\n", "        unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n", "        bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n", "        print(\"# '{}' category:\".format(Product))\n", "        print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n", "        print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n\n", "    #WARNING: these pickles are intended to be printed out only in tuning and test phase, they will be removed later.\n\n", "    #X_train\n", "    with open('Pickles/X_train.pickle', 'wb') as output:\n", "        pickle.dump(X_train, output)\n\n", "    # X_test    \n", "    with open('Pickles/X_test.pickle', 'wb') as output:\n", "        pickle.dump(X_test, output)\n", "        \n", "    # y_train\n", "    with open('Pickles/y_train.pickle', 'wb') as output:\n", "        pickle.dump(y_train, output)\n", "        \n", "    # y_test\n", "    with open('Pickles/y_test.pickle', 'wb') as output:\n", "        pickle.dump(y_test, output)\n", "        \n", "    # df\n", "    with open('Pickles/df.pickle', 'wb') as output:\n", "        pickle.dump(df, output)\n", "        \n", "    # features_train\n", "    with open('Pickles/features_train.pickle', 'wb') as output:\n", "        pickle.dump(features_train, output)\n\n", "    # labels_train\n", "    with open('Pickles/labels_train.pickle', 'wb') as output:\n", "        pickle.dump(labels_train, output)\n\n", "    # features_test\n", "    with open('Pickles/features_test.pickle', 'wb') as output:\n", "        pickle.dump(features_test, output)\n\n", "    # labels_test\n", "    with open('Pickles/labels_test.pickle', 'wb') as output:\n", "        pickle.dump(labels_test, output)\n", "        \n", "    # TF-IDF object\n", "    with open('Pickles/tfidf.pickle', 'wb') as output:\n", "        pickle.dump(tfidf, output)\n", "        \n", "    # Dataframe\n", "    path_df = \"Pickles/df.pickle\"\n", "    with open(path_df, 'rb') as data:\n", "        df = pickle.load(data)\n\n", "    # features_train\n", "    path_features_train = \"Pickles/features_train.pickle\"\n", "    with open(path_features_train, 'rb') as data:\n", "        features_train = pickle.load(data)\n\n", "    # labels_train\n", "    path_labels_train = \"Pickles/labels_train.pickle\"\n", "    with open(path_labels_train, 'rb') as data:\n", "        labels_train = pickle.load(data)\n\n", "    # features_test\n", "    path_features_test = \"Pickles/features_test.pickle\"\n", "    with open(path_features_test, 'rb') as data:\n", "        features_test = pickle.load(data)\n\n", "    # labels_test\n", "    path_labels_test = \"Pickles/labels_test.pickle\"\n", "    with open(path_labels_test, 'rb') as data:\n", "        labels_test = pickle.load(data)\n", "    rf_0 = RandomForestClassifier(random_state = 8)\n\n", "    # n_estimators\n", "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n\n", "    # max_features\n", "    max_features = ['auto', 'sqrt']\n\n", "    # max_depth\n", "    max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n", "    max_depth.append(None)\n\n", "    # min_samples_split\n", "    min_samples_split = [2, 5, 10]\n\n", "    # min_samples_leaf\n", "    min_samples_leaf = [1, 2, 4]\n\n", "    # bootstrap\n", "    bootstrap = [True, False]\n\n", "    # Create the random grid\n", "    random_grid = {'n_estimators': n_estimators,\n", "                'max_features': max_features,\n", "                'max_depth': max_depth,\n", "                'min_samples_split': min_samples_split,\n", "                'min_samples_leaf': min_samples_leaf,\n", "                'bootstrap': bootstrap}\n", "    pprint(random_grid)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # First create the base model to tune\n", "    rfc = RandomForestClassifier(random_state=8)\n\n", "    # Definition of the random search\n", "    random_search = RandomizedSearchCV(estimator=rfc,\n", "                                    param_distributions=random_grid,\n", "                                    n_iter=50,\n", "                                    scoring='accuracy',\n", "                                    cv=3, \n", "                                    verbose=1, \n", "                                    random_state=8)\n\n", "    # Fit the random search model\n", "    random_search.fit(features_train, labels_train)\n", "    print(\"The best hyperparameters from Random Search are:\")\n", "    print(random_search.best_params_)\n", "    print(\"\")\n", "    print(\"The mean accuracy of a model with these hyperparameters is:\")\n", "    print(random_search.best_score_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}